# the cicd pipeline, involvs testing, security scans, ecr upload and  eventually kubernetes deployment

name: CICD Pipeline

#triggers
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

#in real world scenario would use different branching strategy.
#for now i will be focusing on a single one (trunk based) in order to simplify workflow.


env:
  AWS_REGION: us-east-1
  BACKEND_ECR_REGISTRY: ${{ secrets.BACKEND_ECR_REGISTRY }}
  FRONTEND_ECR_REGISTRY: ${{ secrets.FRONTEND_ECR_REGISTRY }}
  BACKEND_IMAGE: iai-task-backend
  FRONTEND_IMAGE: iai-task-frontend


  #i've used sigstore's OIDC in order to authenticate the images, so we'll have keyless signing,
  #which wont require managment, and its also identity based (like gh actions can prove the repo and commit it run),
  #and lastly, the logs are saved in rekor which is a public transperency log.
  #gh has oidc token ->cosign issue token against CA (fulcia) - >fulcio issues a temp cert -> the image is signed and the whole proccess is logged
permissions:
  contents: read
  id-token: write  # needed for cosign keyless signing 
  packages: write
  security-events: write
  

jobs:

  #of that as a company standard i would also implement a client side one on each 
  #of the dev's machine (as part of a post OS installation script of some sort), but in order to
  #prevent further exposure of secrets, i thought it would be wise to scan and fail before uplaoding 
  #an exploitable image
  
  gitleaks:
    name: Secret Scanning Server Side
    runs-on: ubuntu-latest
    steps:
      - name: Checkout SCM
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # fetch full history for gitleaks
      
      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  
  backend:
    name: Backend CI
    runs-on: ubuntu-latest
    needs: [gitleaks]
    defaults:
      run:
        working-directory: ./backend
    
    steps:
      # get the code first
      - name: Checkout SCM
        uses: actions/checkout@v4
      
      # setup python for running tests
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      # install deps and run your test
      - name: Run tests
        run: |
          pip install -r requirements.txt
          python test.py
      - name: Linting
        run: |
          pip install pylint
          pylint . --fail-under=5 --recursive=y # so it can run on the whole /backend dir
          
      # build the docker image and tag with commit hash for easier traceability if something goes wrong (we'll have a refrence)
      - name: Building the image
        run: |
          docker build -t ${{ env.BACKEND_IMAGE }}:${{ github.sha }} .
          docker tag ${{ env.BACKEND_IMAGE }}:${{ github.sha }} ${{ env.BACKEND_IMAGE }}:latest
          
      
       #scan image for vulns before pushing to ecr
      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.BACKEND_IMAGE }}:${{ github.sha }}
          format: 'sarif' 
          output: 'backend-trivy-results.sarif'
          severity: 'CRITICAL' #where we set like the security gate failure
          # fail pipeline if vulns found
          # exit-code: '1'  
          # chose to skip and continue with the project, instead of fix vulns for the demo
          
      # we would want to audit everything 
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()  # upload even if scan fails
        with:
          sarif_file: 'backend-trivy-results.sarif'

      # generate software bill of materials 
      # we would like to secure our chain of supply and ensure that we'll identify vulns quickly and adjust accordignly,
      # as well as ensure software transparency
      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.BACKEND_IMAGE }}:${{ github.sha }}
          format: 'spdx-json'
          output-file: 'backend-sbom.spdx.json'
      
      - name: Upload SBOM as an artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-sbom
          path: backend-sbom.spdx.json
      
      # only push to ecr on main branch pushes and not not on other branches or PRs (to not spam the building machines)
      - name: Configure aws creds
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Logging into ECR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: aws-actions/amazon-ecr-login@v2
      
      # installing cosign for signing images
      - name: Install Cosign
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: sigstore/cosign-installer@v3
      
      # push image and sign it with keyless signing (uses github oidc). im pushing and then signing since cosign's keyless signing requires 
      # the image to be present in the registry, thus if its not present, the action would be able to occur
      - name: PUSH then SIGN to ECR 
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          docker tag ${{ env.BACKEND_IMAGE }}:${{ github.sha }} ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:${{ github.sha }}
          docker tag ${{ env.BACKEND_IMAGE }}:${{ github.sha }} ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:latest
          docker push ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:${{ github.sha }}
          docker push ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:latest
          cosign sign --yes ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:${{ github.sha }}
          cosign sign --yes ${{ env.BACKEND_ECR_REGISTRY }}/${{ env.BACKEND_IMAGE }}:latest
    
  frontend:
    name: Frontend CI
    runs-on: ubuntu-latest
    needs: [gitleaks]
    defaults:
      run:
        working-directory: ./frontend
    
    steps:
      - name: Checkout SCM
        uses: actions/checkout@v4
      
      # setup node for running tests
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: './frontend/package-lock.json'
      
      # install npm packages
      - name: Install dependencies
        run: npm ci

      - name: Building the image
        run: |
          docker build -t ${{ env.FRONTEND_IMAGE }}:${{ github.sha }} .
          docker tag ${{ env.FRONTEND_IMAGE }}:${{ github.sha }} ${{ env.FRONTEND_IMAGE }}:latest
          
      
      # run the test script (simple /health check)
      - name: Run tests
        run: |
          docker run -d -p 8080:8080 --name test-frontend ${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          sleep 10
          docker logs test-frontend
          docker ps -a | grep test-frontend
          chmod +x test.sh && ./test.sh || true
          docker stop test-frontend
      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          format: 'sarif' 
          output: 'frontend-trivy-results.sarif'
          severity: 'CRITICAL' 
          exit-code: '1'
          
          
     
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()  # upload even if scan fails
        with:
          sarif_file: 'frontend-trivy-results.sarif'

      
      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          format: 'spdx-json'
          output-file: 'frontend-sbom.spdx.json'
      
      - name: Upload SBOM as an artifact
        uses: actions/upload-artifact@v4
        with:
          name: frontend-sbom
          path: frontend-sbom.spdx.json
      
      - name: Configure aws creds
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Logging into ECR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Install Cosign
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: sigstore/cosign-installer@v3
      
      - name: Sign then push to ECR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          docker tag ${{ env.FRONTEND_IMAGE }}:${{ github.sha }} ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          docker tag ${{ env.FRONTEND_IMAGE }}:${{ github.sha }} ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:latest
          docker push ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          docker push ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:latest
          cosign sign --yes ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:${{ github.sha }}
          cosign sign --yes ${{ env.FRONTEND_ECR_REGISTRY }}/${{ env.FRONTEND_IMAGE }}:latest
          
  deploy:
    name: Deploy to cluster
    runs-on: self-hosted
    needs: [backend, frontend]  # dependant on be and fe tasks
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Load kubeconfig from param store
        run: |
          aws ssm get-parameter --name "kubeconfig" --with-decryption \
            --query "Parameter.Value" --output text > $HOME/.kube/config
          kubectl --request-timeout=5s auth can-i get pods -A
          
      - name: Checkout SCM
        uses: actions/checkout@v4
          
      - name: Verify connection
        run: kubectl cluster-info #i'll just use the host's already configured kubeconfig
      
      # create ecr pull secrets in both fe and be namespaces
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Create ECR secrets
        run: |
          # get ecr token
          TOKEN=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})
          
          # create or update secret in ns (ecr's secrets rotate every 12h, so we'll need to fetch one each deployment)
          #idempotent ns creation so it wont fail the pipeline: generate to a yml, then apply it
          
          kubectl create namespace backend --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n backend create secret docker-registry ecr-secret --docker-server="${{ env.BACKEND_ECR_REGISTRY }}" --docker-username=AWS --docker-password="${TOKEN}" --dry-run=client -o yaml | kubectl -n backend apply -f -

          #idempotent secreet creation so that the old (still valid) secret wont be needed to be deleted
          #and also to make the pipeline continue even if theres already one

          kubectl create namespace frontend --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n frontend create secret docker-registry ecr-secret --docker-server="${{ env.FRONTEND_ECR_REGISTRY }}" --docker-username=AWS --docker-password="${TOKEN}" --dry-run=client -o yaml | kubectl -n frontend apply -f -
          
      - name: Get backend image digest
        id: be
        env:
          REGISTRY: ${{ env.BACKEND_ECR_REGISTRY }}
          REPO: ${{ env.BACKEND_IMAGE }}
          TAG: ${{ github.sha }}
        run: |
          DIGEST=$(aws ecr describe-images \
            --repository-name "${REPO}" \
            --image-ids imageTag="${TAG}" \
            --query 'imageDetails[0].imageDigest' \
            --output text)
          [ -n "${DIGEST}" ] && [ "${DIGEST}" != "None" ] || { echo "no digest"; exit 1; }
          DIG_IMG_REF="${REGISTRY}/${REPO}@${DIGEST}"
          echo "fq_ref=${DIG_IMG_REF}" >> "$GITHUB_OUTPUT"

          # exporting fully qualified img to steps for digest check
          # fail if missing/None

          
      - name: Install Cosign
        uses: sigstore/cosign-installer@v3

        
      - name: Verify backend image signature (cosign)
        run: |
          cosign verify \
            --certificate-oidc-issuer "https://token.actions.githubusercontent.com" \
            --certificate-identity-regexp "https://github.com/Yinonlevy94/IAI(/|$).*" \
            "${{ steps.be.outputs.fq_ref }}"

          # only trust sigs that were made by github's OIDC identity provider
          # and i used regex solution for flucio's identity string 
          
      - name: Deploy backend
        env:
          GIT_SHA: ${{ github.sha }}
          BACKEND_REGISTRY: ${{ env.BACKEND_ECR_REGISTRY }}
          BACKEND_IMAGE: ${{ env.BACKEND_IMAGE }}
        run: |
          envsubst < backend/backend-deployment.yml | kubectl -n backend apply -f -
          kubectl -n backend apply -f backend/service.yml
          kubectl -n backend rollout status deploy/backend --timeout=3m

      
      - name: Deploy frontend
        env: 
          GIT_SHA: ${{ github.sha }}
          FRONTEND_REGISTRY: ${{ env.FRONTEND_ECR_REGISTRY }}
          FRONTEND_IMAGE: ${{ env.FRONTEND_IMAGE }}
        run: |
          envsubst < frontend/frontend-deployment.yml | kubectl -n frontend apply -f -
          envsubst < frontend/service+ingress.yml | kubectl -n frontend apply -f -
          kubectl -n frontend rollout status deploy/frontend --timeout=3m
      
      # verify deployment
      - name: Verify deployment
        run: |
          echo "######### be pods #########"
          kubectl get pods -n backend
          echo "######### fe pods #########"
          kubectl get pods -n frontend
          echo "######### ingress rule #########"
          kubectl get ingress -n frontend
